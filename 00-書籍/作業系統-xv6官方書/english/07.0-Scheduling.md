

# Chapter 7

# Scheduling

Any operating system is likely to run with more processes than the computer has CPUs, so a plan
is needed to time-share the CPUs among the processes. Ideally the sharing would be transparent to
user processes. A common approach is to provide each process with the illusion that it has its own
virtual CPU bymultiplexingthe processes onto the hardware CPUs. This chapter explains how xv6
achieves this multiplexing.

### 7.1 Multiplexing

Xv6 multiplexes by switching each CPU from one process to another in two situations. First, xv6’s
sleepandwakeupmechanism switches when a process makes a system call that blocks (has to
wait for an event), typically inread,wait, orsleep. Second, xv6 periodically forces a switch
to cope with processes that compute for long periods without blocking. The former are voluntary
switches; the latter are called involuntary. This multiplexing creates the illusion that each process
has its own CPU.

Implementing multiplexing poses a few challenges. First, how to switch from one process to
another? The basic idea is to save and restore CPU registers, though the fact that this cannot be
expressed in C makes it tricky. Second, how to force switches in a way that is transparent to user
processes? Xv6 uses the standard technique in which a hardware timer’s interrupts drive context
switches. Third, all of the CPUs switch among the same set of processes, so a locking plan is
necessary to avoid races. Fourth, a process’s memory and other resources must be freed when the
process exits, but it cannot do all of this itself because (for example) it can’t free its own kernel stack
while still using it. Fifth, each CPU of a multi-core machine must remember which process it is
executing so that system calls affect the correct process’s kernel state. Finally,sleepandwakeup
allow a process to give up the CPU and wait to be woken up by another process or interrupt. Care
is needed to avoid races that result in the loss of wakeup notifications.


```
Kernel
```
```
shell cat
```
```
user
space
```
```
kernel
space kstack
shell
```
```
kstack
cat
```
```
kstack
scheduler
```
```
save
swtch swtch restore
```
Figure 7.1: Switching from one user process to another. In this example, xv6 runs with one CPU
(and thus one scheduler thread).

### 7.2 Code: Context switching

Figure 7.1 outlines the steps involved in switching from one user process to another: a trap (system
call or interrupt) from user space to the old process’s kernel thread, a context switch to the current
CPU’s scheduler thread, a context switch to a new process’s kernel thread, and a trap return to the
user-level process. Xv6 has separate threads (saved registers and stacks) in which to execute the
scheduler because it is not safe for the scheduler to execute on any process’s kernel stack: some
other CPU might wake the process up and run it, and it would be a disaster to use the same stack
on two different CPUs. There is a separate scheduler thread for each CPU to cope with situations
in which more than one CPU is running a process that wants to give up the CPU. In this section
we’ll examine the mechanics of switching between a kernel thread and a scheduler thread.

Switching from one thread to another involves saving the old thread’s CPU registers, and restor-
ing the previously-saved registers of the new thread; the fact that the stack pointer and program
counter are saved and restored means that the CPU will switch stacks and switch what code it is
executing.

The functionswtchsaves and restores registers for a kernel thread switch.swtchdoesn’t
directly know about threads; it just saves and restores sets of RISC-V registers, calledcontexts.
When it is time for a process to give up the CPU, the process’s kernel thread callsswtchto
save its own context and restore the scheduler’s context. Each context is contained in astruct
context(kernel/proc.h:2), itself contained in a process’sstruct procor a CPU’sstruct cpu.
swtchtakes two arguments:struct context*oldandstruct context*new. It saves the
current registers inold, loads registers fromnew, and returns.

Let’s follow a process throughswtchinto the scheduler. We saw in Chapter 4 that one possibil-
ity at the end of an interrupt is thatusertrapcallsyield.yieldin turn callssched, which calls
swtchto save the current context inp->contextand switch to the scheduler context previously
saved incpu->context(kernel/proc.c:506).
swtch(kernel/swtch.S:3)saves only callee-saved registers; the C compiler generates code in the
caller to save caller-saved registers on the stack.swtchknows the offset of each register’s field
instruct context. It does not save the program counter. Instead,swtchsaves theraregister,


which holds the return address from whichswtchwas called. Nowswtchrestores registers from
the new context, which holds register values saved by a previousswtch. Whenswtchreturns, it
returns to the instructions pointed to by the restoredraregister, that is, the instruction from which
the new thread previously calledswtch. In addition, it returns on the new thread’s stack, since
that’s where the restoredsppoints.
In our example,schedcalledswtchto switch tocpu->context, the per-CPU scheduler
context. That context was saved at the point in the past whenschedulercalledswtch(ker-
nel/proc.c:466)to switch to the process that’s now giving up the CPU. When theswtchwe have
been tracing returns, it returns not toschedbut toscheduler, with the stack pointer in the current
CPU’s scheduler stack.

### 7.3 Code: Scheduling

The last section looked at the low-level details ofswtch; now let’s takeswtchas a given and
examine switching from one process’s kernel thread through the scheduler to another process. The
scheduler exists in the form of a special thread per CPU, each running theschedulerfunc-
tion. This function is in charge of choosing which process to run next. A process that wants
to give up the CPU must acquire its own process lockp->lock, release any other locks it is
holding, update its own state (p->state), and then callsched. You can see this sequence in
yield(kernel/proc.c:512),sleepandexit.scheddouble-checks some of those requirements
(kernel/proc.c:496-501)and then checks an implication: since a lock is held, interrupts should be
disabled. Finally,schedcallsswtchto save the current context inp->context and switch
to the scheduler context incpu->context.swtchreturns on the scheduler’s stack as though
scheduler’sswtchhad returned(kernel/proc.c:466). The scheduler continues itsforloop, finds
a process to run, switches to it, and the cycle repeats.
We just saw that xv6 holdsp->lockacross calls toswtch: the caller ofswtchmust already
hold the lock, and control of the lock passes to the switched-to code. This arrangement is unusual:
it’s more common for the thread that acquires a lock to also release it. Xv6’s context switching must
break this convention becausep->lockprotects invariants on the process’sstateandcontext
fields that are not true while executing inswtch. For example, ifp->lockwere not held during
swtch, a different CPU might decide to run the process afteryieldhad set its state toRUNNABLE,
but beforeswtchcaused it to stop using its own kernel stack. The result would be two CPUs
running on the same stack, which would cause chaos. Onceyieldhas started to modify a running
process’s state to make itRUNNABLE,p->lockmust remain held until the invariants are restored:
the earliest correct release point is afterscheduler(running on its own stack) clearsc->proc.
Similarly, onceschedulerstarts to convert aRUNNABLEprocess toRUNNING, the lock cannot be
released until the process’s kernel thread is completely running (after theswtch, for example in
yield).
The only place a kernel thread gives up its CPU is insched, and it always switches to
the same location inscheduler, which (almost) always switches to some kernel thread that
previously calledsched. Thus, if one were to print out the line numbers where xv6 switches
threads, one would observe the following simple pattern:(kernel/proc.c:466),(kernel/proc.c:506),


(kernel/proc.c:466),(kernel/proc.c:506), and so on. Procedures that intentionally transfer control to
each other via thread switch are sometimes referred to ascoroutines; in this example,schedand
schedulerare co-routines of each other.

There is one case when the scheduler’s call toswtchdoes not end up insched.allocproc
sets the contextraregister of a new process toforkret(kernel/proc.c:524), so that its firstswtch
“returns” to the start of that function.forkretexists to release thep->lock; otherwise, since
the new process needs to return to user space as if returning fromfork, it could instead start at
usertrapret.

scheduler(kernel/proc.c:445)runs a loop: find a process to run, run it until it yields, repeat.
The scheduler loops over the process table looking for a runnable process, one that hasp->state
== RUNNABLE. Once it finds a process, it sets the per-CPU current process variablec->proc,
marks the process asRUNNING, and then callsswtchto start running it(kernel/proc.c:461-466).

### 7.4 Code: mycpu and myproc

Xv6 often needs a pointer to the current process’sprocstructure. On a uniprocessor one could
have a global variable pointing to the currentproc. This doesn’t work on a multi-core machine,
since each CPU executes a different process. The way to solve this problem is to exploit the fact
that each CPU has its own set of registers.

While a given CPU is executing in the kernel, xv6 ensures that the CPU’stpregister always
holds the CPU’s hartid. RISC-V numbers its CPUs, giving each a uniquehartid.mycpu(ker-
nel/proc.c:74)usestpto index an array ofcpustructures and return the one for the current CPU. A
struct cpu(kernel/proc.h:22)holds a pointer to theprocstructure of the process currently run-
ning on that CPU (if any), saved registers for the CPU’s scheduler thread, and the count of nested
spinlocks needed to manage interrupt disabling.

Ensuring that a CPU’stpholds the CPU’s hartid is a little involved, since user code is free to
modifytp.startsets thetpregister early in the CPU’s boot sequence, while still in machine
mode(kernel/start.c:45).usertrapretsavestpin the trampoline page, in case user code modifies
it. Finally,uservecrestores that savedtpwhen entering the kernel from user space(kernel/trampo-
line.S:78). The compiler guarantees never to modifytpin kernel code. It would be more convenient
if xv6 could ask the RISC-V hardware for the current hartid whenever needed, but RISC-V allows
that only in machine mode, not in supervisor mode.
The return values ofcpuidandmycpuare fragile: if the timer were to interrupt and cause the
thread to yield and later resume execution on a different CPU, a previously returned value would
no longer be correct. To avoid this problem, xv6 requires that callers disable interrupts, and only
enable them after they finish using the returnedstruct cpu.
The functionmyproc(kernel/proc.c:83)returns thestruct procpointer for the process that
is running on the current CPU.myprocdisables interrupts, invokesmycpu, fetches the current
process pointer (c->proc) out of thestruct cpu, and then enables interrupts. The return value
ofmyprocis safe to use even if interrupts are enabled: if a timer interrupt moves the calling process
to a different CPU, itsstruct procpointer will stay the same.


### 7.5 Sleep and wakeup

```
Scheduling and locks help conceal the actions of one thread from another, but we also need ab-
stractions that help threads intentionally interact. For example, the reader of a pipe in xv6 may need
to wait for a writing process to produce data; a parent’s call towaitmay need to wait for a child
to exit; and a process reading the disk needs to wait for the disk hardware to finish the read. The
xv6 kernel uses a mechanism called sleep and wakeup in these situations (and many others). Sleep
allows a kernel thread to wait for a specific event; another thread can call wakeup to indicate that
threads waiting for a specified event should resume. Sleep and wakeup are often calledsequence
coordinationorconditional synchronizationmechanisms.
Sleep and wakeup provide a relatively low-level synchronization interface. To motivate the
way they work in xv6, we’ll use them to build a higher-level synchronization mechanism called
asemaphore[5] that coordinates producers and consumers (xv6 does not use semaphores). A
semaphore maintains a count and provides two operations. The “V” operation (for the producer)
increments the count. The “P” operation (for the consumer) waits until the count is non-zero,
and then decrements it and returns. If there were only one producer thread and one consumer
thread, and they executed on different CPUs, and the compiler didn’t optimize too aggressively,
this implementation would be correct:
```
100 struct semaphore {
101 struct spinlock lock;
102 int count;
103 };
104
105 void
106 V(struct semaphore *s)
107 {
108 acquire(&s->lock);
109 s->count += 1;
110 release(&s->lock);
111 }
112
113 void
114 P(struct semaphore *s)
115 {
116 while(s->count == 0)
117 ;
118 acquire(&s->lock);
119 s->count -= 1;
120 release(&s->lock);
121 }

```
The implementation above is expensive. If the producer acts rarely, the consumer will spend
most of its time spinning in thewhileloop hoping for a non-zero count. The consumer’s CPU
could probably find more productive work thanbusy waitingby repeatedlypollings->count.
```

```
Avoiding busy waiting requires a way for the consumer to yield the CPU and resume only afterV
increments the count.
Here’s a step in that direction, though as we will see it is not enough. Let’s imagine a pair of
calls,sleepandwakeup, that work as follows.sleep(chan)waits for an event designated by
the value ofchan, called thewait channel.sleepputs the calling process to sleep, releasing the
CPU for other work.wakeup(chan)wakes all processes that are in calls tosleepwith the same
chan(if any), causing theirsleepcalls to return. If no processes are waiting onchan,wakeup
does nothing. We can change the semaphore implementation to usesleepandwakeup(changes
highlighted in yellow):
```
200 void
201 V(struct semaphore *s)
202 {
203 acquire(&s->lock);
204 s->count += 1;
205 wakeup(s);
206 release(&s->lock);
207 }
208
209 void
210 P(struct semaphore *s)
211 {
212 while(s->count == 0)
213 sleep(s);
214 acquire(&s->lock);
215 s->count -= 1;
216 release(&s->lock);
217 }

```
Pnow gives up the CPU instead of spinning, which is nice. However, it turns out not to be
straightforward to designsleepandwakeupwith this interface without suffering from what is
known as thelost wake-upproblem. Suppose thatPfinds thats->count == 0on line 212. While
Pis between lines 212 and 213,Vruns on another CPU: it changess->countto be nonzero and
callswakeup, which finds no processes sleeping and thus does nothing. NowPcontinues executing
at line 213: it callssleepand goes to sleep. This causes a problem:Pis asleep waiting for aVcall
that has already happened. Unless we get lucky and the producer callsVagain, the consumer will
wait forever even though the count is non-zero.
The root of this problem is that the invariant thatPsleeps only whens->count == 0is violated
byVrunning at just the wrong moment. An incorrect way to protect the invariant would be to move
the lock acquisition (highlighted in yellow below) inPso that its check of the count and its call to
sleepare atomic:
```
300 void
301 V(struct semaphore *s)
302 {
303 acquire(&s->lock);


304 s->count += 1;
305 wakeup(s);
306 release(&s->lock);
307 }
308
309 void
310 P(struct semaphore *s)
311 {
312 acquire(&s->lock);
313 while(s->count == 0)
314 sleep(s);
315 s->count -= 1;
316 release(&s->lock);
317 }

```
One might hope that this version ofPwould avoid the lost wakeup because the lock preventsV
from executing between lines 313 and 314. It does that, but it also deadlocks:Pholds the lock
while it sleeps, soVwill block forever waiting for the lock.
We’ll fix the preceding scheme by changingsleep’s interface: the caller must pass thecon-
dition locktosleepso it can release the lock after the calling process is marked as asleep and
waiting on the sleep channel. The lock will force a concurrentVto wait untilPhas finished putting
itself to sleep, so that thewakeupwill find the sleeping consumer and wake it up. Once the con-
sumer is awake againsleepreacquires the lock before returning. Our new correct sleep/wakeup
scheme is usable as follows (change highlighted in yellow):
```
400 void
401 V(struct semaphore *s)
402 {
403 acquire(&s->lock);
404 s->count += 1;
405 wakeup(s);
406 release(&s->lock);
407 }
408
409 void
410 P(struct semaphore *s)
411 {
412 acquire(&s->lock);
413 while(s->count == 0)
414 sleep(s, &s->lock);
415 s->count -= 1;
416 release(&s->lock);
417 }

```
The fact thatPholdss->lockpreventsVfrom trying to wake it up betweenP’s check of
s->countand its call tosleep. However,sleepmust releases->lockand put the consuming
```

process to sleep in a way that’s atomic from the point of view ofwakeup, in order to avoid lost
wakeups.

### 7.6 Code: Sleep and wakeup

Xv6’ssleep(kernel/proc.c:548)andwakeup(kernel/proc.c:579)provide the interface used in the
last example above. The basic idea is to havesleepmark the current process asSLEEPINGand
then callschedto release the CPU;wakeuplooks for a process sleeping on the given wait channel
and marks it asRUNNABLE. Callers ofsleepandwakeupcan use any mutually convenient number
as the channel. Xv6 often uses the address of a kernel data structure involved in the waiting.
sleepacquiresp->lock(kernel/proc.c:559)andonly thenreleaseslk. As we’ll see, the fact
thatsleepholds one or the other of these locks at all times is what prevents a concurrentwakeup
(which must acquire and hold both) from acting. Now thatsleepholds justp->lock, it can put
the process to sleep by recording the sleep channel, changing the process state toSLEEPING, and
callingsched(kernel/proc.c:563-566). In a moment it will be clear why it’s critical thatp->lockis
not released (byscheduler) until after the process is markedSLEEPING.
At some point, a process will acquire the condition lock, set the condition that the sleeper
is waiting for, and callwakeup(chan). It’s important thatwakeupis called while holding the
condition lock^1 .wakeuploops over the process table(kernel/proc.c:579). It acquires thep->lock
of each process it inspects. Whenwakeupfinds a process in stateSLEEPINGwith a matching
chan, it changes that process’s state toRUNNABLE. The next timeschedulerruns, it will see that
the process is ready to be run.
Why do the locking rules forsleepandwakeupensure that a process that’s going to sleep
won’t miss a concurrent wakeup? The going-to-sleep process holds either the condition lock or
its ownp->lockor both frombeforeit checks the condition untilafterit is markedSLEEPING.
The process callingwakeupholdsbothlocks inwakeup’s loop. Thus the waker either makes the
condition true before the consuming thread checks the condition; or the waker’swakeupexamines
the sleeping thread strictly after it has been markedSLEEPING. Thenwakeupwill see the sleeping
process and wake it up (unless something else wakes it up first).
Sometimes multiple processes are sleeping on the same channel; for example, more than one
process reading from a pipe. A single call towakeupwill wake them all up. One of them will run
first and acquire the lock thatsleepwas called with, and (in the case of pipes) read whatever data
is waiting. The other processes will find that, despite being woken up, there is no data to be read.
From their point of view the wakeup was “spurious,” and they must sleep again. For this reason
sleepis always called inside a loop that checks the condition.
No harm is done if two uses of sleep/wakeup accidentally choose the same channel: they will
see spurious wakeups, but looping as described above will tolerate this problem. Much of the
charm of sleep/wakeup is that it is both lightweight (no need to create special data structures to
act as sleep channels) and provides a layer of indirection (callers need not know which specific
process they are interacting with).

(^1) Strictly speaking it is sufficient ifwakeupmerely follows theacquire(that is, one could callwakeupafter
therelease).


### 7.7 Code: Pipes

A more complex example that usessleepandwakeupto synchronize producers and consumers
is xv6’s implementation of pipes. We saw the interface for pipes in Chapter 1: bytes written to one
end of a pipe are copied to an in-kernel buffer and then can be read from the other end of the pipe.
Future chapters will examine the file descriptor support surrounding pipes, but let’s look now at
the implementations ofpipewriteandpiperead.
Each pipe is represented by astruct pipe, which contains alockand adatabuffer.
The fieldsnreadand nwritecount the total number of bytes read from and written to the
buffer. The buffer wraps around: the next byte written afterbuf[PIPESIZE-1]isbuf[0]. The
counts do not wrap. This convention lets the implementation distinguish a full buffer (nwrite ==
nread+PIPESIZE) from an empty buffer (nwrite == nread), but it means that indexing into
the buffer must usebuf[nread % PIPESIZE]instead of justbuf[nread](and similarly for
nwrite).
Let’s suppose that calls topipereadandpipewritehappen simultaneously on two different
CPUs.pipewrite(kernel/pipe.c:77)begins by acquiring the pipe’s lock, which protects the counts,
the data, and their associated invariants.piperead(kernel/pipe.c:106)then tries to acquire the lock
too, but cannot. It spins inacquire(kernel/spinlock.c:22)waiting for the lock. Whilepiperead
waits,pipewriteloops over the bytes being written (addr[0..n-1]), adding each to the pipe in
turn(kernel/pipe.c:95). During this loop, it could happen that the buffer fills(kernel/pipe.c:88). In this
case,pipewritecallswakeupto alert any sleeping readers to the fact that there is data waiting
in the buffer and then sleeps on&pi->nwriteto wait for a reader to take some bytes out of the
buffer.sleepreleases the pipe’s lock as part of puttingpipewrite’s process to sleep.
pipereadnow acquires the pipe’s lock and enters its critical section: it finds thatpi->nread
!= pi->nwrite(kernel/pipe.c:113)(pipewritewent to sleep becausepi->nwrite == pi->nread
+ PIPESIZE(kernel/pipe.c:88)), so it falls through to theforloop, copies data out of the pipe(ker-
nel/pipe.c:120), and incrementsnreadby the number of bytes copied. That many bytes are now
available for writing, sopipereadcallswakeup(kernel/pipe.c:127)to wake any sleeping writers
before it returns.wakeupfinds a process sleeping on&pi->nwrite, the process that was running
pipewritebut stopped when the buffer filled. It marks that process asRUNNABLE.
The pipe code uses separate sleep channels for reader and writer (pi->nreadandpi->nwrite);
this might make the system more efficient in the unlikely event that there are lots of readers and
writers waiting for the same pipe. The pipe code sleeps inside a loop checking the sleep condition;
if there are multiple readers or writers, all but the first process to wake up will see the condition is
still false and sleep again.

### 7.8 Code: Wait, exit, and kill

sleepandwakeupcan be used for many kinds of waiting. An interesting example, introduced
in Chapter 1, is the interaction between a child’sexitand its parent’swait. At the time of the
child’s death, the parent may already be sleeping inwait, or may be doing something else; in the
latter case, a subsequent call towaitmust observe the child’s death, perhaps long after it calls


exit. The way that xv6 records the child’s demise untilwaitobserves it is forexitto put the
caller into theZOMBIEstate, where it stays until the parent’swaitnotices it, changes the child’s
state toUNUSED, copies the child’s exit status, and returns the child’s process ID to the parent. If
the parent exits before the child, the parent gives the child to theinitprocess, which perpetually
callswait; thus every child has a parent to clean up after it. A challenge is to avoid races and
deadlock between simultaneous parent and childwaitandexit, as well as simultaneousexit
andexit.

waitstarts by acquiringwait_lock(kernel/proc.c:391), which acts as the condition lock that
helps ensure thatwaitdoesn’t miss awakeupfrom an exiting child. Thenwaitscans the process
table. If it finds a child inZOMBIEstate, it frees that child’s resources and itsprocstructure,
copies the child’s exit status to the address supplied towait(if it is not 0), and returns the child’s
process ID. Ifwaitfinds children but none have exited, it callssleepto wait for any of them
to exit(kernel/proc.c:433), then scans again.waitoften holds two locks,wait_lockand some
process’spp->lock; the deadlock-avoiding order is firstwait_lockand thenpp->lock.

exit(kernel/proc.c:347)records the exit status, frees some resources, callsreparentto give
its children to theinitprocess, wakes up the parent in case it is inwait, marks the caller as
a zombie, and permanently yields the CPU.exitholds bothwait_lockandp->lockduring
this sequence. It holdswait_lockbecause it’s the condition lock for thewakeup(p->parent),
preventing a parent inwaitfrom losing the wakeup.exitmust holdp->lockfor this sequence
also, to prevent a parent inwaitfrom seeing that the child is in stateZOMBIEbefore the child has
finally calledswtch.exitacquires these locks in the same order aswaitto avoid deadlock.

It may look incorrect forexitto wake up the parent before setting its state toZOMBIE, but that
is safe: althoughwakeupmay cause the parent to run, the loop inwaitcannot examine the child
until the child’sp->lockis released byscheduler, sowaitcan’t look at the exiting process
until well afterexithas set its state toZOMBIE(kernel/proc.c:379).

Whileexitallows a process to terminate itself,kill(kernel/proc.c:598)lets one process re-
quest that another terminate. It would be too complex forkillto directly destroy the victim
process, since the victim might be executing on another CPU, perhaps in the middle of a sensitive
sequence of updates to kernel data structures. Thuskilldoes very little: it just sets the victim’s
p->killedand, if it is sleeping, wakes it up. Eventually the victim will enter or leave the kernel,
at which point code inusertrapwill callexitifp->killedis set (it checks by callingkilled
(kernel/proc.c:627)). If the victim is running in user space, it will soon enter the kernel by making a
system call or because the timer (or some other device) interrupts.

If the victim process is insleep,kill’s call towakeupwill cause the victim to return from
sleep. This is potentially dangerous because the condition being waited for for may not be true.
However, xv6 calls tosleepare always wrapped in awhileloop that re-tests the condition after
sleepreturns. Some calls tosleepalso testp->killedin the loop, and abandon the current
activity if it is set. This is only done when such abandonment would be correct. For example, the
pipe read and write code(kernel/pipe.c:84)returns if the killed flag is set; eventually the code will
return back to trap, which will again checkp->killedand exit.

Some xv6sleeploops do not checkp->killedbecause the code is in the middle of a multi-
step system call that should be atomic. The virtio driver(kernel/virtio_disk.c:285)is an example: it


does not checkp->killedbecause a disk operation may be one of a set of writes that are all
needed in order for the file system to be left in a correct state. A process that is killed while waiting
for disk I/O won’t exit until it completes the current system call andusertrapsees the killed flag.

### 7.9 Process Locking

The lock associated with each process (p->lock) is the most complex lock in xv6. A simple
way to think aboutp->lockis that it must be held while reading or writing any of the following
struct procfields:p->state,p->chan,p->killed,p->xstate, andp->pid. These fields
can be used by other processes, or by scheduler threads on other CPUs, so it’s natural that they
must be protected by a lock.
However, most uses ofp->lockare protecting higher-level aspects of xv6’s process data struc-
tures and algorithms. Here’s the full set of things thatp->lockdoes:

- Along withp->state, it prevents races in allocatingproc[]slots for new processes.
- It conceals a process from view while it is being created or destroyed.
- It prevents a parent’swaitfrom collecting a process that has set its state toZOMBIEbut has
    not yet yielded the CPU.
- It prevents another CPU’s scheduler from deciding to run a yielding process after it sets its
    state toRUNNABLEbut before it finishesswtch.
- It ensures that only one CPU’s scheduler decides to run aRUNNABLEprocesses.
- It prevents a timer interrupt from causing a process to yield while it is inswtch.
- Along with the condition lock, it helps preventwakeupfrom overlooking a process that is
    callingsleepbut has not finished yielding the CPU.
- It prevents the victim process ofkillfrom exiting and perhaps being re-allocated between
    kill’s check ofp->pidand settingp->killed.
- It makeskill’s check and write ofp->stateatomic.

Thep->parentfield is protected by the global lockwait_lockrather than byp->lock.
Only a process’s parent modifiesp->parent, though the field is read both by the process it-
self and by other processes searching for their children. The purpose ofwait_lockis to act as
the condition lock whenwaitsleeps waiting for any child to exit. An exiting child holds either
wait_lockorp->lockuntil after it has set its state toZOMBIE, woken up its parent, and yielded
the CPU.wait_lockalso serializes concurrentexits by a parent and child, so that theinit
process (which inherits the child) is guaranteed to be woken up from itswait.wait_lockis a
global lock rather than a per-process lock in each parent, because, until a process acquires it, it
cannot know who its parent is.


### 7.10 Real world

The xv6 scheduler implements a simple scheduling policy, which runs each process in turn. This
policy is calledround robin. Real operating systems implement more sophisticated policies that,
for example, allow processes to have priorities. The idea is that a runnable high-priority process
will be preferred by the scheduler over a runnable low-priority process. These policies can become
complex quickly because there are often competing goals: for example, the operating system might
also want to guarantee fairness and high throughput. In addition, complex policies may lead to
unintended interactions such aspriority inversionandconvoys. Priority inversion can happen when
a low-priority and high-priority process both use a particular lock, which when acquired by the
low-priority process can prevent the high-priority process from making progress. A long convoy
of waiting processes can form when many high-priority processes are waiting for a low-priority
process that acquires a shared lock; once a convoy has formed it can persist for long time. To avoid
these kinds of problems additional mechanisms are necessary in sophisticated schedulers.
sleepandwakeupare a simple and effective synchronization method, but there are many
others. The first challenge in all of them is to avoid the “lost wakeups” problem we saw at the
beginning of the chapter. The original Unix kernel’ssleepsimply disabled interrupts, which suf-
ficed because Unix ran on a single-CPU system. Because xv6 runs on multiprocessors, it adds
an explicit lock tosleep. FreeBSD’smsleeptakes the same approach. Plan 9’ssleepuses a
callback function that runs with the scheduling lock held just before going to sleep; the function
serves as a last-minute check of the sleep condition, to avoid lost wakeups. The Linux kernel’s
sleepuses an explicit process queue, called a wait queue, instead of a wait channel; the queue has
its own internal lock.
Scanning the entire set of processes inwakeupis inefficient. A better solution is to replace
thechanin bothsleepandwakeupwith a data structure that holds a list of processes sleeping
on that structure, such as Linux’s wait queue. Plan 9’ssleepandwakeupcall that structure a
rendezvous point. Many thread libraries refer to the same structure as a condition variable; in that
context, the operationssleepandwakeupare calledwaitandsignal. All of these mechanisms
share the same flavor: the sleep condition is protected by some kind of lock dropped atomically
during sleep.
The implementation ofwakeupwakes up all processes that are waiting on a particular chan-
nel, and it might be the case that many processes are waiting for that particular channel. The
operating system will schedule all these processes and they will race to check the sleep condition.
Processes that behave in this way are sometimes called athundering herd, and it is best avoided.
Most condition variables have two primitives forwakeup:signal, which wakes up one process,
andbroadcast, which wakes up all waiting processes.
Semaphores are often used for synchronization. The count typically corresponds to something
like the number of bytes available in a pipe buffer or the number of zombie children that a process
has. Using an explicit count as part of the abstraction avoids the “lost wakeup” problem: there is
an explicit count of the number of wakeups that have occurred. The count also avoids the spurious
wakeup and thundering herd problems.
Terminating processes and cleaning them up introduces much complexity in xv6. In most op-
erating systems it is even more complex, because, for example, the victim process may be deep


inside the kernel sleeping, and unwinding its stack requires care, since each function on the call
stack may need to do some clean-up. Some languages help out by providing an exception mecha-
nism, but not C. Furthermore, there are other events that can cause a sleeping process to be woken
up, even though the event it is waiting for has not happened yet. For example, when a Unix process
is sleeping, another process may send asignalto it. In this case, the process will return from the
interrupted system call with the value -1 and with the error code set to EINTR. The application
can check for these values and decide what to do. Xv6 doesn’t support signals and this complexity
doesn’t arise.
Xv6’s support forkillis not entirely satisfactory: there are sleep loops which probably should
check forp->killed. A related problem is that, even forsleeploops that checkp->killed,
there is a race betweensleepandkill; the latter may setp->killedand try to wake up the
victim just after the victim’s loop checksp->killedbut before it callssleep. If this problem
occurs, the victim won’t notice thep->killeduntil the condition it is waiting for occurs. This
may be quite a bit later or even never (e.g., if the victim is waiting for input from the console, but
the user doesn’t type any input).
A real operating system would find freeprocstructures with an explicit free list in constant
time instead of the linear-time search inallocproc; xv6 uses the linear scan for simplicity.

### 7.11 Exercises

1. Implement semaphores in xv6 without usingsleepandwakeup(but it is OK to use spin
    locks). Choose a few of xv6’s uses of sleep and wakeup and replace them with semaphores.
    Judge the result.
2. Fix the race mentioned above betweenkillandsleep, so that akillthat occurs after
    the victim’s sleep loop checksp->killedbut before it callssleepresults in the victim
    abandoning the current system call.
3. Design a plan so that every sleep loop checksp->killedso that, for example, a process
    that is in the virtio driver can return quickly from the while loop if it is killed by another
    process.
4. Modify xv6 to use only one context switch when switching from one process’s kernel thread
    to another, rather than switching through the scheduler thread. The yielding thread will need
    to select the next thread itself and callswtch. The challenges will be to prevent multiple
    CPUs from executing the same thread accidentally; to get the locking right; and to avoid
    deadlocks.
5. Modify xv6’sschedulerto use the RISC-VWFI(wait for interrupt) instruction when no
    processes are runnable. Try to ensure that, any time there are runnable processes waiting to
    run, no CPUs are pausing inWFI.
